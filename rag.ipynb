{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to RAG Applications\n",
    "\n",
    "This notebook creates a simple RAG (Retrieval-Augmented Generation) system to answer questions from a PDF document using an open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"LexMachina.pdf\"\n",
    "\n",
    "# We'll be using Llama 3.1 8B for this example.\n",
    "MODEL = \"llama3.2:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF document\n",
    "\n",
    "Let's start by loading the PDF document and breaking it down into separate pages.\n",
    "\n",
    "<img src='images/documents.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.4 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (0.3.4)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (0.3.13)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (2.6.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain<0.4.0,>=0.3.4->langchain_community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain_community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from pypdf) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 4\n",
      "Length of a page: 923\n",
      "Content of a page: 3. Technical Details\n",
      "Architecture&Workflow:Thearchitectureof LexMachinacomprisesseveral components:\n",
      "● Frontend:DevelopedentirelywithFlutterallowingseamlessdeployment acrossmobile(Android, iOS), web, anddesktop.● Backend:Node.jswithExpresshandleshighthroughput andintegratesLangChainfornatural languageprocessingandRAG. Queriesareprocessedthroughthebackend,whichinteractswithlocallyhostedLLaMAmodels. K3sorchestratesbackenddeployment onprivatecloudoron-premises.● AI Model Integration:Meta’sLlama3.2(11Band90BVisionInstruct)forlegalquestions, document interpretation, andPDFextraction.● Database&CloudStorage:Self-hostedPostgreSQLforreal-timedatabase, MinIOforstorage, andOAuth2forauthentication, ensuringprivacy.\n",
      "DataSources:\n",
      "● PubliclyAvailableLegal Texts: Laws, regulations, government notifications, andlegalsummariesfromofficial sources.● UserContributions: Suggestionsfromcommunityusersforupdatedlegalinterpretations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages in chunks\n",
    "\n",
    "Pages are too long, so let's split pages into different chunks.\n",
    "\n",
    "<img src='images/splitter.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "Length of a chunk: 922\n",
      "Content of a chunk: 3. Technical Details\n",
      "Architecture&Workflow:Thearchitectureof LexMachinacomprisesseveral components:\n",
      "● Frontend:DevelopedentirelywithFlutterallowingseamlessdeployment acrossmobile(Android, iOS), web, anddesktop.● Backend:Node.jswithExpresshandleshighthroughput andintegratesLangChainfornatural languageprocessingandRAG. Queriesareprocessedthroughthebackend,whichinteractswithlocallyhostedLLaMAmodels. K3sorchestratesbackenddeployment onprivatecloudoron-premises.● AI Model Integration:Meta’sLlama3.2(11Band90BVisionInstruct)forlegalquestions, document interpretation, andPDFextraction.● Database&CloudStorage:Self-hostedPostgreSQLforreal-timedatabase, MinIOforstorage, andOAuth2forauthentication, ensuringprivacy.\n",
      "DataSources:\n",
      "● PubliclyAvailableLegal Texts: Laws, regulations, government notifications, andlegalsummariesfromofficial sources.● UserContributions: Suggestionsfromcommunityusersforupdatedlegalinterpretations.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the chunks in a vector store\n",
    "\n",
    "We can now generate embeddings for every chunk and store them in a vector store.\n",
    "\n",
    "<img src='images/vectorstore.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-ollama) (0.3.13)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-ollama) (0.3.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.1.137)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from ollama<1,>=0.3.0->langchain-ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.10.10)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.2.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\karth\\documents\\development\\gentle-intro-to-rag\\.venv\\lib\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a retriever\n",
    "\n",
    "We can use a retriever to find chunks in the vector store that are similar to a supplied question.\n",
    "\n",
    "<img src='images/retriever.png' width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'LexMachina.pdf', 'page': 3}, page_content=\"5. Impact&Potential\\nTargetAudience:\\n● Citizensseekingeasyaccesstolegal information.● Legal Professionalslookingforaquickreferencetool.● Students&ResearchersstudyingIndianlaw.\\nImpact&Benefits:\\n● Social Impact: Empowercitizenswithknowledgeof theirrights, reducingrelianceonlegal intermediaries.● EconomicImpact: Lowerlegal costsbyprovidingaccessiblelegal information.\\nScalability&FuturePlans:\\n● FutureFeatures: Integrateacommunityreviewsystemforlegal updates. Developvoice-basedsearchanddocument analysis.● Global Expansion: Expandtoincludelegal systemsof othercountries.\\n6. ConclusionLexMachinaismorethananAI tool, it'sabeaconofhopeforthoseoverwhelmedbytheIndianlegal system. Bysimplifyingcomplexlawsandbreakingdownlanguagebarriers, weempowercitizenstounderstandandassert theirrights. LexMachinabridgesthegapbetweenpeopleandjustice, turningconfusionintoclarityandhelplessnessintoempowerment. Webelievethat whenknowledgeisaccessible, justicebecomesattainable. Together, wecantransformlives, onelegal queryat atime.\\n7. ContactInformation\\nTeamLeadEmail:karthidreamr@gmail.comForfurtherinquiriesorcollaboration, pleasereachout.\"),\n",
       " Document(metadata={'source': 'LexMachina.pdf', 'page': 1}, page_content='3. Technical Details\\nArchitecture&Workflow:Thearchitectureof LexMachinacomprisesseveral components:\\n● Frontend:DevelopedentirelywithFlutterallowingseamlessdeployment acrossmobile(Android, iOS), web, anddesktop.● Backend:Node.jswithExpresshandleshighthroughput andintegratesLangChainfornatural languageprocessingandRAG. Queriesareprocessedthroughthebackend,whichinteractswithlocallyhostedLLaMAmodels. K3sorchestratesbackenddeployment onprivatecloudoron-premises.● AI Model Integration:Meta’sLlama3.2(11Band90BVisionInstruct)forlegalquestions, document interpretation, andPDFextraction.● Database&CloudStorage:Self-hostedPostgreSQLforreal-timedatabase, MinIOforstorage, andOAuth2forauthentication, ensuringprivacy.\\nDataSources:\\n● PubliclyAvailableLegal Texts: Laws, regulations, government notifications, andlegalsummariesfromofficial sources.● UserContributions: Suggestionsfromcommunityusersforupdatedlegalinterpretations.'),\n",
       " Document(metadata={'source': 'LexMachina.pdf', 'page': 2}, page_content='4. Features&Functionality\\nKeyFeatures:\\n● Retrieval-AugmentedGeneration(RAG)foraccurateresponseswithsourcecitations● OCRwithNLPforparsingcomplexscannedandhandwrittendocuments● Cross-platformsupport:Mobile, Desktop, Web● HybridSearch(Dense+SparseRetrieval)forimprovedqueryaccuracy● Legal InformationAccess: AI-drivenexplanationsof legal questions, withRAGtocitereliablesources.● DocumentSupport: Abilitytoparseandextract informationfromimages, PDFs, andotherfileformats.\\nUserInteraction:\\n● Theapplicationisavailableacrossall platformslikeMobile, Web, Desktopetc., fromtheirrespectiveappstores.● Userscaninputlegal questionsoruploaddocumentsdirectlywithintheapp,receivingclear, easy-to-understandresultspoweredbyAI.Theintuitiveinterfaceensuresseamlessnavigationforall users, regardlessof legal knowledge.\\nInnovation:\\n● Pioneerstheintegrationof cutting-edgeAI specificallytailoredfortheIndianlegalsystem.● BridgeslanguagebarriersbysupportingmultipleIndianlanguages, enhancingaccessibility.● UtilisesAI tointerpret diversecontentformats, includingcomplexandhandwrittendocuments.● Empowerscitizensbyreducingrelianceonintermediariesthroughdirect accesstolegal knowledge.● Adoptsacommunity-driven, open-sourceapproachforcontinual updatesandrelevance.'),\n",
       " Document(metadata={'source': 'LexMachina.pdf', 'page': 0}, page_content='AI HackathonwithMetaLlama\\n1. TeamInformation\\n● TeamName:LexMachina● TeamMembers:○ KarthikeyanN(SoftwareArchitect)○ SurendarKS(SoftwareEngineer)● ContactEmail:karthidreamr@gmail.com\\n2. ProjectOverview\\nProjectTitle:IndianLawGenerativeAIHackathonTheme:AI forSocietal Good\\nProblemStatement:\\n● InIndia, understandinglegal lawsisoftencomplexandintimidatingfortheaveragecitizen. Theheavyrelianceonlegal intermediaries, suchaslawyers, createsasignificant barriertoaccessingjustice, oftenleadingtohighcostsandlongtimetoprovidejustice.● Manyindividualsareunawareoftheirrightsandthelegal processesinvolved.Moreover, crucial legal informationisscatteredacrossmultiplesources, makingitdifficult toaccessaccurateandup-to-datecontent.● Compoundingthisissue, muchof thelegal frameworkisbasedonEnglishcommonlaw, whichposesachallengefornon-Englishspeakers. Thelackof comprehensivemultilingual supportfurtherlimitsaccessibility, makingit harderforpeopletoengagewithlegal knowledgeintheirnativelanguages.\\nSolutionOverview:\\n● LexMachinaisanAI-poweredplatformdesignedtodemocratiseaccesstoIndianlegal information.● Simplifiescomplexlegal languageandofferslanguagetranslation, makingitunderstandablefortheaveragecitizen.● LeveragesadvancedAI modelstointerpret legal content frombothtextual andvisualsources.● Providesauser-friendlyinterfaceaccessibleacrossmultipleplatforms.● Ensurestransparencyandreliabilitybydeliveringaccurateinformationwithsourcecitations.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\" Technical Details of LexMachina \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n",
    "\n",
    "We'll be using Ollama to load the local model in memory. After creating the model, we can invoke it with a question to get the response back.\n",
    "\n",
    "<img src='images/model.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Lex Machina is a company that provides data and analytics for intellectual property (IP) litigation. They offer a platform that allows users to analyze and compare the performance of attorneys, law firms, and judges in patent cases.\\n\\nThe platform uses data from publicly available sources, such as court filings and transcripts, to provide insights on various aspects of IP litigation, including:\\n\\n1. Attorney performance: Lex Machina analyzes the success rates, win-loss records, and other metrics for individual attorneys and law firms.\\n2. Judge performance: The platform evaluates judges' decisions and outcomes in patent cases, providing information on their track record and consistency.\\n3. Case strategy: Lex Machina offers data-driven insights on case strategies, including the use of expert witnesses, discovery tactics, and settlement negotiations.\\n\\nBy analyzing this data, users can gain a better understanding of the IP litigation landscape, identify trends and patterns, and make more informed decisions about their own cases or investments in IP litigation.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2024-10-30T06:24:24.9613792Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10090384100, 'load_duration': 25136800, 'prompt_eval_count': 32, 'prompt_eval_duration': 391659000, 'eval_count': 193, 'eval_duration': 9677879000}, id='run-6e96d6bd-887f-4cc8-b945-e7357ebe4f25-0', usage_metadata={'input_tokens': 32, 'output_tokens': 193, 'total_tokens': 225})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=MODEL, temperature=0)\n",
    "model.invoke(\"What's LexMachina?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I couldn\\'t find any information on \"LexMachina.\" It\\'s possible that it\\'s a fictional or non-existent entity, or it may be a term that is not widely known or used.\\n\\nIf you could provide more context or clarify what you mean by \"LexMachina,\" I\\'d be happy to try and help you further.', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2024-10-30T06:25:27.4963824Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3260488700, 'load_duration': 55021800, 'prompt_eval_count': 37, 'prompt_eval_duration': 436804000, 'eval_count': 70, 'eval_duration': 2767226000}, id='run-3f169f12-ea65-4b70-b7bd-461d81832c35-0', usage_metadata={'input_tokens': 37, 'output_tokens': 70, 'total_tokens': 107})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\" What's the technical details of LexMachina? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the model's response\n",
    "\n",
    "The response from the model is an `AIMessage` instance containing the answer. We can extract the text answer by using the appropriate output parser. We can connect the model and the parser using a chain.\n",
    "\n",
    "<img src='images/parser.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any information on \"LexMachina.\" It's possible that it's a fictional or non-existent entity, or it may be a term that is not widely known or used.\n",
      "\n",
      "If you could provide more context or clarify what you mean by \"LexMachina,\" I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "print(chain.invoke(\"What's the technical details of LexMachina ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also want to provide the model with the context from the PDF file. We can use a prompt template to define and reuse the prompt we'll use with the model.\n",
    "\n",
    "\n",
    "<img src='images/prompt.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the prompt to the chain\n",
    "\n",
    "We can now chain the prompt with the model and the parser.\n",
    "\n",
    "<img src='images/chain1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the retriever to the chain\n",
    "\n",
    "Finally, we can connect the retriever to the chain to get the context from the vector store.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the chain to answer questions\n",
    "\n",
    "Finally, we can use the chain to ask questions that will be answered using the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's the technical details of LexMachina ?\n",
      "Answer: The technical details of LexMachina are:\n",
      "\n",
      "* Architecture & Workflow:\n",
      "\t+ Frontend: Developed entirely with Flutter for seamless deployment across mobile (Android, iOS), web, and desktop.\n",
      "\t+ Backend: Node.js with Express handles high throughput and integrates LangChain for natural language processing and RAG.\n",
      "\t+ AI Model Integration: Meta's Llama 3.2 (11B) and 90B Vision Instruct for legal questions, document interpretation, and PDF extraction.\n",
      "* Database & Cloud Storage:\n",
      "\t+ Self-hosted PostgreSQL for real-time database\n",
      "\t+ MinIO for storage\n",
      "\t+ OAuth2 for authentication, ensuring privacy\n",
      "*************************\n",
      "\n",
      "Question: What's the problem LexMachina solves ?\n",
      "Answer: LexMachina solves the problem of complex and intimidating access to Indian legal laws for the average citizen, reducing reliance on legal intermediaries and providing accessible legal information.\n",
      "*************************\n",
      "\n",
      "Question: Who's US President ?\n",
      "Answer: I don't know.\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What's the technical details of LexMachina ?\",\n",
    "    \"What's the problem LexMachina solves ?\",\n",
    "    \"Who's US President ?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
